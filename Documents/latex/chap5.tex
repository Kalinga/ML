%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Simulation, Results and Discussion}
In this chapter results related to various types of experiments with different parameters are presented and discussed.

\section{LSTM results}
\subsection{LSTM results}
LSTM model is trained with the videos 71-346. Video data from 317-346, 30 videos data is used for model testing. Normally performance of a future bounding box predictor evaluated  in two ways. 
(i) It is determined by the center location error, it computes the average
euclidean distance between the center position of the tracked targets and the ground truth
positions. This is computed for all the frames. But this However if the predictor predicts 
output location sometimes randomly, then the average center location error value does not measure the
true performance correctly. In our experimented we have used averaged center location error. \\

(ii) To measure how precisely the predictions are, Intersection over the Union (IoU) is used. In our experiment we tried with several values for IoU. When a particular value for the IoU is considered, the measured IOU value greater than the particular value in the question is treated as positive.

\begin{figure}[H]
\includegraphics[scale=1.0]{conf7_1000e_15ffuture}
\begin{center}
\caption{Training and validation loss for - 15 frame input and prediction at 15 time step }
\end{center}
\end{figure}

%Test RMSE represents testing the model with frames extracted from one video from the test set.
%conf1: model with epochs = 1000, MinMaxScaler(feature\textunderscore range=(0, 1)), 
%number of sequence as observation = 3
%loss: 0.0045 - val\textunderscore loss: 0.0047 
%Test RMSE: 2.363
%Validation: Using one pedestrian bounding boxes with 205 frames.
% git commit 789d6e6

%conf2: 
%model with epochs = 200, early stopping after epoch 33
%number of sequence as observation = 15
%loss: 0.0087 - val\textunderscore loss: 0.0089
%Test RMSE: 4.749
%9c88a25

%conf3:
%model with epochs = 200, early stopping after epoch 88
%loss-0.0062 val\textunderscore loss-0.0070.h5
%model with epochs = 200
%number of sequence as observation = 30
%Test RMSE: 4.625


%conf 4  single layer, epochs 200 without early stopping
%Test data set (30 videos)
%IoU > 0.5
%avg prediction speed 0.0161
%Avg accuracy 0.39

%IoU > 0.45
%avg prediction 0.0163
%Avg accuracy 0.4416

%IoU > 0.25
%avg prediction 0.0163
%Avg accuracy 0.6603
%************************************
%conf 4 Train data set
%IoU > 0.5 0.0164 0.4483

%IoU > 0.45 0.0162 0.5106

%IoU > 0.25 0.74

%conf 5 Test data set

%val_loss improved from 0.04356 to 0.04316, saving model to conf5_epoch-281_loss-0.0296_val_loss-0.0432.h5
%Epoch 282/300
% - 999s - loss: 0.0295 - val_loss: 0.0425

%conf 5 Test data set (30 videos)
%model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))
%model.add(Dropout(0.1))

%3 more layers

%IoU > 0.25 0.0625 0.627

%IoU > 0.45 0.0626 0.444

%IoU > 0.50 0.062 0.389

%conf 6 Test data set (30 videos)
%trained for 1000epochs
%IoU > 0.50
%0.016 0.351

%IoU > 0.45
%0.016 0.410

%IoU > 0.25
%0.016 0.636


%Conf7:
%epoch 1000
%input seq 15
%predicting 15th future frame

%They can change their walking direction in an instance,
%or start/stop walking abruptly. As a consequence, sensible prediction horizons
%are typical short (we consider < 2s in this paper).

%ls cm-pm | wc -l 185      *2  370
%ls cm-ps | wc -l 142 145  *3  335
%ls cms-pm | wc -l 245 245 *2  490
%ls cms-ps | wc -l 37 40   *12 480
%ls cs-pm| wc -l 173 175   *2  350
%ls cs-ps | wc -l 15 15    *30 450

%Train on 342135 samples, validate on 85534 samples
%Epoch 1/300
%Started at 17th sept 15:45 

%Annotations for the video can be extracted as below
%anno = imdb.\textunderscore get\textunderscore annotations(vid), this results in a dictionary which contains below keys ['height', 'ped\textunderscore annotations', 'num\textunderscore frames', 'width']

%TODO
%\newpara Weakly supervised learning!
%Sigmoid cross entropy vs  softmax
%batch normalization

%\section{LSTM with refinement}
\section{Discussion}

\section{SSD results}
\subsection{Validation}
Below graphs depicts the validation error when SSD model was trained for 10 epochs.
\begin{figure}[H]
\includegraphics[scale=0.4]{conf0_loss-val_loss_0_10epochs}
\begin{center}
\caption{Training and Validation error at 10 epochs}
\end{center}
\end{figure}
As the training error has not converged after 10 epochs it was decided to training for another 10 epoch and the graph as follows. After training for 10 epochs the training error gradually reduces and the validation error revolves around training error as shown in the below graph. As the training of single epoch takes significant time the statistics were taken with small epochs.

\begin{figure}[H]
\includegraphics[scale=0.4]{conf0_loss-val_loss_10_20epochs}
\begin{center}
\caption{Training and Validation error at 20 epochs}
\end{center}
\end{figure}

\newpara
Looking at the graph which depicts training loss and validation loss drawn with several epochs, we could see training error gradually reduces with increase in the number of epochs, simultaneously we see validation loss also exhibiting same pattern and slowly decreasing with number of epochs. This trend of decrease in validation error with increase in training is a good indication of learning. The model could have trained for some more epochs until it converges, however due to long training time, it was not possible to do so, instead training with different configuration was done to see the behavior with other configuration.

\subsection{Testing} 
For the testing of the generated model below steps were followed to prepare the test data and conduct the experiment.
\begin{enumerate}
	\item video set 06 - 10 was selected as test data and bounding box, label are extracted into labels\textunderscore test\textunderscore full.csv which contains total 154436 records
	\item Rows with occluded and other classes except person were removed and remaining number of records were 73218, which consists of 42910 unique images 
	\item records with less than 50 pixel for height or 5 pixel for width were discarded and data set left with 29620 records
	\item From the above data 1000 records were randomly chosen for test purpose which consists of 902 unique images
\end{enumerate}

\newpara
While using SSD7, it was observed that, the algorithm is very sensitive to the list of aspect ratios for the anchor boxes. To begin with, the values for aspect\textunderscore ratios = $[0.5, 1.0, 2.0]$ were used and that lead to very poor result in the evaluation phase. Subsequently the model was trained with several different configuration and results are noted as below.

for aspect\textunderscore ratios = $[0.1, 0.2, 0.33, 0.413, 0.418, 0.5, 0.6, 0.7, 0.8, 1.0]$
\begin{table}[H]
\begin{center}
 \begin{tabular}{||c c c||} 
 \hline
 No. epoch & mAP & fps\\ [0.8ex] 
 \hline\hline
 10 & 0.661 & 10.22\\ 
 \hline
 20  & 0.635 & 9.73 \\
\hline
\end{tabular}
\caption{Average Precision and fps with number of epochs}
\end{center}
\end{table}
From the above result, it can be observed that training the data for more epochs actually not resulting a better model as both mAP and fps is lowered for the model trained with 20 epochs, this could be sign of \textit{over-fitting.}

%conf1
%0.2, 0.33, 0.413, 0.418, 0.5, 0.6, 0.7, 0.8, 1.0
%0.4376
%fps 73.14

%conf 2
%epoch 5
%0.1, 0.2, 0.33, 0.413, 0.418, 0.5, 0.6, 0.7, 0.8
%fps: 75.58 
%mAP:0.471

%conf 3 
%0.1, 0.2, 0.33, 0.413, 0.418, 0.5, 0.6, 0.7, 0.8, 1.0

With the increase in number of values in the list of aspect ratio, the number of predictor boxes increases. Increase in number of predictor boxes increases both training time as well as prediction time.
