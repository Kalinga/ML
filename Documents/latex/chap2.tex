%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{State of the Art}
% object detection, pedestrain detection
%\cite{felzenszwalb2009object, walk2010new, liu2016ssd, szegedy2014scalable, dollar2009pedestrian, dollar2011pedestrian}

% feature extraction
%\cite{dalal2005histograms, lowe2004distinctive}

% Region Proposal
%\cite{girshick2014rich, girshick2015fast, ren2015faster}

% Trackers
%\cite{fiaz2019handcrafted,xu2005pedestrian,ning2017spatially}

% Activity prediction
%\cite{ryoo2011human, richard2017weakly }


% Intention prediction
%\cite{saleh2017intent, fang2018pedestrian, abu2018will, dollar2005behavior, cao2017realtime, rasouli2017agreeing }

% Trajectory prediction
%\cite{saleh2017intent, zhang2019sr, xue2018ss, lipton2015critical}

The problem of human action classification or pedestrian intent prediction, pedestrain future trajectory estimation \cite{saleh2017intent, fang2018pedestrian, abu2018will, dollar2005behavior, rasouli2017agreeing, cao2017realtime} from a sequence of images has got huge attention from the researchers in recent years. This problem is inherently complex as it is based on other complex problems. To predict future location, detect the action or intention one must first detect the object in an image and classify it, track the same object over several continuous frames. Before the dominance of convolutional neural network (CNN) and Deep Learning which has outperformed other traditional methods in the recent times. For any of such prediction task we need an efficient and robust object detection, object localization and object tracking model working seamlessly together to enable last phase of the prediction task. Object detection and localization constitute the back bone of this pipe line and takes maximum share of the run time in the entire pipeline. To build a real time pedestrian future bounding box prediction pipeline, it is utmost important to find a model that performs real time object detection \cite{felzenszwalb2009object, walk2010new, liu2016ssd, szegedy2014scalable, dollar2009pedestrian, dollar2011pedestrian} and localization reliably. In this chapter some of the state-of-the-art models for such tasks is explored.

\section{Pedestrian intention on roads}
\newpara Abu Farha and team presented \cite{abu2018will} a work, where they discussed methods to predicts action that are considerably in future and their duration. And they tried to anticipate all activities within a horizon of 5 minutes. After inferring the activities from the observed part of the video using an RNN-HMM \cite{richard2017weakly}, they proposed two approaches to predict the future actions and their durations, in the first approach they used RNN, where the anticipated activities are fed to the RNN to predict the remaining duration of the on going activities and duration of the next activity along with its class.
In the second approach the proposed CNN based single pass model which predicts length and label of the future action. They have found that these both approach have outperformed both grammar based baseline and nearest neighbor baseline. RNN and CNN found to be performing similarly for time horizon more than 40 seconds and RNN performs better for lesser than 20 seconds time horizon. The task is more formally defined as,
given the first t frames $X_{\text{1}}^t$ \\
predict \[ C_{t+1}^T  = \ (C_{t+1}, ..., C_{T}) \]
where $C_{\text{i}}$ denotes action labels for unobserved frames
and the video is given by
\[ X_{1}^T = (X_{1}, ..., X_{T}) \]

\newpara
For the RNN training the loss function used as below
\begin{equation}
    L = -log\, \hat{p_c} + (l_r - \hat{l_r})^2 +  I (l_n - \hat{l_n})^2 
\end{equation}
where \\
$\hat{l_r}$ represents predicted remaining length of the current action, \\
$\hat{l_n}$ represents predicted length of the next action, \\
$\hat{p_c}$ is the predicted class of the next action

\newpara Unlike recursive strategy in RNN, CNN approach does the prediction in a one single pass.
Training data generated by using first 10\%, 20\%, 30\%, 50\% of the video as the observation and the following 50\% as the ground truth. During the training of the network squared error used as loss function.
\begin{equation}
    L = \frac{1} {SC} \sum_{a,c} (Y_{sc} - \hat{Y_{sc}})^2 
\end{equation}
where $\hat{Y}$ is the prediction of the network. \\
C represents action classes and \\
S number of rows for an action segment

\newpara Row-wise $\textit{l}_2$-normalization \footnote{$l_2$-norm of a real vector $x=(x_1,x_2,x_3)$ is given by $|x|=sqrt(x_1^2+x_2^2+x_3^2)$} of the output found to be more robust than softmax output with cross-entropy loss. \cite{abu2018will} did not elaborate the 'input sequence always end 1 second before the next action segment start'. What is the purpose of this and results if the input sequence is provided till the next action segment starts.

For the task of early detection, the goal is to recognize the activity with least possible amount of input observation \cite{ryoo2011human}. In \cite{ryoo2011human} they modeled the feature distribution over the course of observation by integral histogram representation of activities, and named the prediction algorithm as \textit{dynamic bag-of-words} as the prediction algorithm considers the sequential structure formed by video features. And they formulated the activity prediction process probabilistically as:
%\hat{f}(x,y) = \underset{(s,t)\in S_{xy}}{\mathrm{median}} \{g(s,t)\}
%\begin{equation}
%\begin{multlined}
\begin{align}
\begin{split}
	%\displaystyle \sum_{n=1}^\infty
		P(A_{p}\: |\: O,t) ={}& \displaystyle \sum_{d}  P(A_{p},d\: |\: O,t)\\
		={}&	\frac{\sum_{d} P(O\: | \: A_{p},d)P(t\: | \:d)P(A_{p},\: d)}
	 {\sum_{i}\sum_{d} P(O\: | \: A_{i},d)P(t\: | \:d)P(A_{i},\: d) }
\end{split}
\end{align}
%\end{multlined}
%\end{equation}

\newpara Where \textit{d} is a progress level of the activity \\
\textit{$A_{p}$}  \textit{ P(t|d)} represents the similarity between the observation length \textit{t} and that of the activity progress \textit{d}
\textit{P(O|Ap, d)} measures the similarity between the video observation and the activity \textit{$A_{p}$} with progress level \textit{d}. This \textit{integral bag-of-words} uses 3-D space-time local features and detect motion changes in the video and generates descriptors representing local movements in the video. These are cuboid feature descriptors \cite{dollar2005behavior}. After the features are extracted, \textit{k-means} clustering was applied and the clusters are knows as visual words. Every feature detected is now belong to one of this k-visual words. Then the activities are model as an integral histogram of visual-words. The similarity between a video and activity model was done by comparing their histogram representation. And finally dynamic programming was used to predict the ongoing activities from videos.

\section{Prediction (estimation) methods }
The last stage of the pipeline which actually predicts the future location of the visual object \cite{saleh2017intent, zhang2019sr, xue2018ss, lipton2015critical}. These algorithms predicts  bounding box for a tracked pedestrian or predicts intention of the pedestrian, whether pedestrian wants to cross in near future or not. Subsequent section shall discuss about some of the state-of-the art methodology that address this.
In \cite{saleh2017intent} Saleh et.al presented a data driven approach which is different from older approach which employs dynamical motion modeling and motion planning. They have formulated the the intent prediction problem as a time-series problem. They proposed the method by observing a short window sequence of the pedestrian motion trajectory, a prediction about their future latent position can be done up to 4 secs ahead. With a deep stacked LSTM network they achieved competent results on Daimler testing data set. In their work, three LSTM layers are stacked. The input layer takes 2-dimension sequence with a window size 10 of lateral position of pedestrian motion trajectory. Last LSTM layer connected to a fully connected layer which has only one neuron and predicts lateral position at the next time step. A linear activation function is used in the fully connected layer. In this method, only the next time step value was predicted. At the inference time recursive prediction was used to predict any window sized sequence. Prediction with different size input sequence was achieved by this.
As part of training this LSTM  model, which is an optimization problem, Mean Squared Error (MSE) was used as loss function.
\begin{equation}
MSE= \frac{1}{N}\sum_{i=1}^{N}(\hat{Y_i} - Y_i)^2
\end{equation}
They presented their work on Daimler pedestrian path prediction benchmark dataset, that consists of 68 stereo image sequences. These sequence includes four scenarios 'Crossing', 'Stopping', 'Starting' and 'Bending In'. These sequences are annotated with frame-wise pedestrian bounding boxes, median disparity of the upper body area of the pedestrian, position of the pedestrian in the vehicle coordinate system and time to event information. This method achieved a superior results in both short and long term prediction for the four scenarios with the test data.

In \cite{zhang2019sr}, Zhang et.al presented a work, which includes current intention of the neighbors and iteratively refines the current states of all participants in the crowd using a message passing mechanism. Social behaviors are stressed upon and learned using LSTM, but \cite{zhang2019sr} neglected factors such as current states of neighbors and adapting selected information from neighbors based on their motions and locations. In their work they introduced a State Refined module as an subnetwork of the LSTM cells. This subnetwork aligns pedestrian together and updates their current state. Formulation for the cell state was given by 
\begin{equation}
\hat{C}^{t, l+1}= \sum_{j\in N(i)}M(\hat{h_j}^{t, l}, {h}^{t, l}) + \hat{C}^{t, l}
\end{equation}
Where M is the message passing function, and used to calculate social information from the neighboring pedestrian. They also used \textbf{Pedestrian-wise }attention and \textbf{Motion gate} to select important information from neighboring pedestrian for message passing. However in this paper they have not described its usefulness in the vehicle and Pedestrian context. This motivated me to explore vehicle and pedestrian social awareness use that information.

In \cite{xue2018ss}, Xue et. al used a hierarchical LSTM model for Pedestrian Trajectory Prediction. They have considered the influence of social neighborhood and scene layouts. In their model they used three different LSTMs to capture person, social and scene scale information in a circular shape neighborhood. The dataset and scenarios described in this paper are humans in a crowded place. And it has not explored pedestrian crossing or trying to cross in a traffic scenario. However some of the findings still applicable in the pedestrian-moving vehicle scenario as well.


\section{Features extraction methods}
Object detection cite{felzenszwalb2009object, walk2010new, liu2016ssd, szegedy2014scalable, dollar2009pedestrian, dollar2011pedestrian} is the core for pose estimation and object tracking. The accuracy of this stage very important for other stages to perform. This detection stage identify the object and locates it. In this section various new algorithms are discussed those deals with object detection.
\subsection{Manual feature extraction}
Pedro F. Felzenszwalb and team proposed deformable part model (DPM) \cite{felzenszwalb2009object} which is based on pictorial structures that represent objects by means of a collection of object parts arranged in a deformable configuration. Each object part captures local appearance properties of
an object and the deformable configuration is identified by spring-like connections between certain pairs of such parts. They use a variation of Support Vector Machine(SVM) which they named as latent SVM (LSVM) is used for the training of the model and  histogram of oriented gradients (HOG) used for features vector. After the success of usages of CNNs in tasks like image classification, CNNs are used to extract the feature sets instead of manual extraction.

\newpara \textbf{HOG }
In year 2005 Dalal \& Triggs found a method which is superior to Haar wavelets descriptor and other state-of-the-art method based on edge and gradient, at that time for the task of pedestrian detection. HOG descriptors are comparable with edge orientation histogram, SIFT \footnote{Scale Invariant Feature Transform, by transforming image data into scale invariant co-ordinates relative to local features \cite{lowe2004distinctive}} descriptors and shape context, however these are computed on a dense grid of uniformly spaced cell and use overlapping local contrast normalization. Which they named as Histograms of Oriented Gradient (HOG) features\cite{dalal2005histograms}. The gradient is computed by applying a filter kernel \\
\begin{center}
$[-1,0,1] \, and \, [-1,0,1] ^{T}$
\end{center}

\newpara After computation of the gradient they are gradient values are used to update a 9 histogram channel which are evenly spread over 0-180 degrees or 0-360 degrees. They tried peson/non-person classification using this robust feature descriptor and Linear SVM and got state-of-the-art result of that time.

\subsection{CNN based feature extraction}
\newpara CNNs found to be outperform manual feature extraction as they are principally designed Neural net \textbf{architecture which preserves local connections and shares weights}. CNN are special type of multilayer perceptrons, based shared weight architecture and inspired by biological connectivity between neurons in human visual cortex. A particular neuron respond to stimuli only in a restricted region of the visual field called receptive field. CNNs employs a special kind of linear operation called convolution. CNN is characterized by series of several convolution, non-linearity, pooling operation and finally followed by one or more fully connected layers. Initial convolutional layers are used as feature extraction and final layers as fully connected. Fully connected layer takes the input from convolutional network and produces a an N-dimensional vector, where N is the number of classes model is intended to select.


\section{Region of Interest proposing methods}
Object localization is one step forward when compared to object recognition. Identifying whether an object is present in the image or not is the task of object recognition, however object localization deals with locating where the object is located in an image. There are several approaches used for localization as discussed below. And it is complex in comparison to image classification because huge number of candidate object locations must be processed during the localization process. \\

\textbf{Sliding Window:} A classifier is trained on the objects first and in the next phase a window is moved over whole image at different scales. Each window gets a score from the learned classifier and window(s) with the highest score predicts the location of an object. Sliding Window technique is computationally inefficient. Some of the improved and modern methods are presented below. 

\subsection{R-CNN Introduction}
In \cite{girshick2014rich} Girshick et al. combined region proposals with CNN and naming the method as R-CNN. This method generates around 2000 category independent region proposals for the input image, using a CNN, a fixed length feature vector is extracted regardless of the region shape. Then each region is classified using class specific linear SVM. 

\newpara \textbf{R-CNN:} \\
The features for a region proposal cite{girshick2014rich, girshick2015fast, ren2015faster} is computed via a CNN proposed by Krizhevsky et al. This network takes mean-subtracted 227 x 227 RGB image as input. Therefore image data in the region proposal is converted to 227 x 227 as required by Krizhevsky network. For this arbitrary-shaped  proposed region is dilated and warped. During the test time using selective search fast mode 2000 region proposals are generated and warped and processed through CNN to extract desired features. After that per class basis score is evaluated using the SVM trained for that class. Subsequently a greedy non-max suppression for each class independently is applied which rejects region that has IoU overlap with higher scoring selected region larger than a threshold.

\newpara \textbf{Fast R-CNN:} \\
In \cite{girshick2015fast} an improved technique is discussed which is based on previously discussed R-CNN. This is computationally faster than R-CNN ans results in faster training and testing speed. R-CNN training was a multi-stage pipeline. It first fine tunes a ConvNet on object proposal and then fits SVMs to ConvNet features. Because of feature extraction from each object proposal, without sharing computation in each test image, object detection was relatively slow in R-CNN. Based on the concept described in SPPnets, a convolutional feature map is generated for entire input image and subsequent classification is done by extracting features from the shared feature map for each object proposal. Features for the object proposal is extracted by max-pooling the portion of feature map inside the  object proposal region into a fixed size output. During the training, Fast R-CNN takes an entire image and set of object proposals, for each object proposal an RoI layer extract a fixed length feature vector from the feature map. Each feature vector is further fed into fully connected layers which produces a feature vector that is branched into two output layers, one outputs softmax probability over K object classes and other layer produces four real numbers for each of the K object classes termed as bbox regressor. The architecture is trained with multi-task loss function via back propagation. Fast R-CNN uses softmax classifier instead of linear SVM that was used in R-CNN.

\newpara \textbf{Faster R-CNN:} \\  
Another enhanced proposal came from Author of previously discussed R-CNN methods with a goal of sharing computation with a Fast R-CNN for the task of region proposal. In \cite{ren2015faster}, a new Region Proposal Network (RPN) is introduced that shares full image convolutional features with the detection network, thus almost eliminating the cost for the region proposals. RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. This RPN is trained to generate high quality region proposals, which are used by Fast R-CNN for detection. By sharing convolutional layers with the object detection network, RPN has marginal cost for computing proposal is small \footnote{in the order of 10 ms per image}. For the very deep VGG-16 model this methodology achieved 5fps on GPU and accuracy of 73.2\%mAP for PASCAL VOC 2007 using 300 proposals per image. 

\subsection{Model based on R-CNN}
\newpara Estimating pedestrian future using pedestrian dynamic model were done in the past and these models are difficult to adjust and to achieve robustness, they require high quality stereo data, dense optical flow and ego-motion compensation which requires vehicle data.  Using the 2D pose estimation method, that is applied to the still images in sliding window manner, a state of-the-art result has been obtained\cite{fang2018pedestrian} for the C/NC task with Daimler dataset. In the pipeline of their task they used below components. They have used generic off-the-shelf CNN modules as Detector, Tracker and Pose Estimator in their pipeline.

\begin{itemize}
	\item Detection: Fine tuned Faster R-CNN \cite{ren2015faster} based on VGG16 CNN architecture. 
	\item Tracking: Object tracking-by-detection \cite{wojke2017simple}, purely image based of-the-shelf solution 
	\item Pose Estimation: CNN-based pose estimation method discussed in \cite{cao2017realtime}
	\item Prediction: Random Forest based on 4096T dimensional vector, where T is number of frames tracked
\end{itemize}

\newpara
Researcher Fang and team in their 2018 paper \cite{fang2018pedestrian} described about image based 2D pose estimation for detecting pedestrian intention: whether the pedestrian crossing the road, stopping before entering the road, starting to walk or bending towards the road. They performed their experiment with choreographed Daimler dataset. They also used publicly available, a rather new dataset (JAAD) \cite{kotseruba2016joint} that allows development of methods and experiment in a more naturalistic driving condition. They used CNN based pedestrian detection, tracking and pose estimation to predict the crossing action from monocular images. In their paper they have mentioned that without additional information such as stereo, optical-flow or ego-motion compensation, they achieved the state-of-the-art results with only image-based 2D pose estimation. The prediction of the action is based on per pedestrian multi-frame feature set extracted using last \textit{k} frames. Estimating the pose was central to the prediction of the crossing intention. In the results they have observed that classification with respect to features based on  skeleton of the pedestrian outperform the features based on CNN fc6 layer.

\subsection{Single Shot Multi Box}
There are several proposals proposed for person detection, many of them try to achieve high accuracy at the expense of high computational cost. This leads many of such proposal not useful for real time application such driver-less cars. Single Shot Multi Box methodology aims at faster detection and suitable for real time applications.  
SSD motivated by the ideas presented in \cite{szegedy2014scalable} where a convolutional network is trained to output the coordinates of the object bounding boxes. MultiBox loss is the weighted sum of \textit{confidence} loss and \textit{location} loss. Single Shot Multi Box with aforementioned inspiration attacked the problem with several small improvements and variation achieving state-of-the art results with still simpler architecture, in comparison to \cite{szegedy2014scalable} which uses a separate deep neural network to generate proposal for the bounding box.
\subsubsection{SSD}
Faster R-CNN, operates at only 7 frames per second (FPS) SSD aimed at improving the speed by employing some new methods which does not re-samples pixels or features for the bounding hypothesized boxes. This improves the speed substantially with out much or no decrease in accuracy. SSD was implemented and used with Caltech dataset \cite{dollar2009pedestrian} during thesis period and details shall are discussed in the subsequent chapters.

\newpara The central principle on which SSD \cite{liu2016ssd} is based on is, discretizing the output space of bounding boxes into a set of pre-determined default boxes consisting of different aspect ratios and scale per feature map location. The principle used as part of SSD overcomes the challenge faced in \cite{ren2015faster} and its predecessors which includes multi phase training and slow inference time. SSD does this by doing both the task of object localization and classification in a single forward pass. It uses a \textit{MultiBox} approach, which pre-computes priors \footnote{Alternatively known as anchors in Faster R-CNN}, fixed size bounding boxes. In MultiBox based approach, prediction starts with priors and try to regress closer to the ground truth bounding boxes.
It uses combination of confidence loss and location loss for calculating the loss of the model. Confidence loss uses categorical cross-entropy \footnote{otherwise known as softmax loss} and location loss uses Smooth L1-Norm \footnote{\textit{L1}-norm is well known as \textit{Manhattan }norm}.

\begin{equation}
	\left \| x_1 \right \| =\sum_i(x_{1_i})
\end {equation}

L1-norm gives distance between two vectors, known as \textit{Sum of Absolute Difference} distance given by below equation:
\begin{equation}
	SAD(x_1,x_2) = \left \| x_1-x_2 \right \|_1 = \sum \left | x_{1_i}-x_{2_i} \right |
\end {equation}

\textbf{Fixed Priors:} In case of MultiBox, the priors are chosen according to their IoU with respect to ground truth above a certain defined ratio. \footnote{Usually 0.5 is considered as threshold value}.
However, in case of Fixed Priors, carefully, a set of different size and aspect ratio priors are chosen manually. This lead to removal of pre-training phase for the prior generation. For a feature map with \textit{b} default bounding boxes per cell and model is trained for \textit{c} classes, then the number of values for the feature map is given by the relation 
\begin{equation}
	f = (m * n )  (4 + c) * b
\end {equation}
4 in the above equation indicates number of co-ordinate values for two corner points or 4 offset relative to the original default box shape. \\
m x n is the shape of the feature map \\
 Additional point to note is, more the number of default boxes, the more accurate is the detection at the cost of speed.
Predicting category scores for multiple classes and box offsets for pre-computed bounding boxes using a small convolutional filters applied to feature maps lead to a faster single pass object classification and localization system. And prediction of different scales from feature maps of different scales and separate prediction by aspect ratio leads to higher accuracy. \cite{liu2016ssd} With same VGG-16 base architecture SSD300 model runs at 59 FPS. The mentioned result from SSD paper is very interesting and suitable for application of recurrent neural network to detect and track object in video simultaneously in real time.

\subsubsection{YOLO}
Another variation of single shot detection during the same time achieving the state-of-the-art result was YOLO. Single network based detection pipeline. YOLO divides the image into a grid of cells and predicts the coordinates and confidences of objects contained in the cells. Authors like Szegedy et.al in \cite{szegedy2014scalable} has expressed uncertainty about its performance in the situation where there are significantly more objects in a data set. As SSD seems to performs better with respect to run time and performance, YOLO was not studied further during thesis period.

\section{Tracking methods}
Visual object tracking is also a very active area of research, several algorithms are discussed in \cite{fiaz2019handcrafted,xu2005pedestrian,ning2017spatially}.
In \cite{fiaz2019handcrafted} Fiaz et. al broadly categorized trackers into o Correlation Filter based Trackers (CFTs) and Non-CFTs. Human trackers can be divided into motion model or appearance model. They experimented with 24 recent trackers and found that trackers using deep features performed better than handcrafted. In some cases fusion of both increases performances significantly. Based on their study they have concluded that Discriminative Correlation Filter (DCF) based trackers perform better than others. There are some notable implementation e.g. 
Kalman Filter with Mean shift tracking, Recurrent YOLO based spatially supervised RCNN \cite{ning2017spatially}, a deep neural network that uses raw video frames as input and its output is the coordinates of a bounding box of an object being tracked in each frame. Single night vision camera can also be used in the night for tracking tracking of the pedestrian with help of Kalman filter and mean shift. Some of sensor fusion techniques improves the tracking as well as detection result when IR sensor, RADAR, laser output are processed together with camera image. Fang et.al discussed in their survey paper In \cite{gonzalez2016pedestrian}, combination of visible and non-visible imaging techniques increasing detection accuracy. More in-depth research was out of the scope of the thesis topic, so no further study was done in this area during the thesis period.



