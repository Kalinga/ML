%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\pagestyle{fancy}
\fancyhf{}
%\fancyhead[EL]{\nouppercase\leftmark}
\fancyhead[EL]{\leftmark} % E: even, L:Left, O:Odd, 
\fancyhead[OL]{\leftmark}
\fancyhead[ER,OR]{\thepage}

\pagenumbering{arabic}
\setcounter{page}{1}

\chapter{Introduction}
\section{Motivation}
Deep Learning has made some significant impacts in the areas of image classification, 
object detection, speech recognition, text-to-speech generation, machine translation,
online recommender system, medical diagnosis and few more. With availability of high 
computing power such as high speed CPU and high bandwidth GPU, large amount of 
data from which actionable insight is expected and nice and powerful ecosystem of  
tools such as TensorFlow and PyTorch; many researchers in the scientific community 
starting from neurologist to computer scientist are enticed towards research in the area 
of Artificial Intelligence. Several interesting findings, new algorithms, record 
breaking benchmarking results while applying Deep Learning algorithms has taken 
AI to a different level.

\vspace{1em}
\noindent Unlike human, recognizing and localizing objects still remains a challenge for artificial 
systems, mainly because of two factors - view point dependent object variability and
high in-class variability. In the human detection task intra-class variability includes
color of the clothing, type of clothing, appearance, pose, illumination, partial occlusions
and background. Pedestrian detection has remain most studied problem in the computer vision.
In the last decade with application of CNNs researchers are able to get some nice results 
those are good enough for application in practical scenarios.

\vspace{1em}
\noindent The detection of the human is a partial task where human and machine must interact 
with each other and machine expected to understand human intention. For example, 
a self driving must anticipate and predict the pedestrian in a reliable manner, whether 
the pedestrian intents to cross the road, or standing near the curb or waiting at waiting shelter.
Such intent prediction problem can be viewed as two step related problem. First detect the 
human and track the person in several frames and predict after tracking for certain time using a 
previously trained model for such intention prediction.

\vspace{1em}
\noindent The main idea of this thesis is to take a look at different aspect of solving such a problem, 
starting with reliable data \footnote{There are several image and video datasets exist in the 
public domain for research purpose} acquisition, understanding the annotation in the data, 
training a neural network using efficient features for the detection of person. Detecting the same person in the series of subsequent frames and finally prediction of pedestrian intention using another classifier.

\section{Problem Statement} 
Which model to be used for the first part of the problem, which deals with detecting the pedestrians in the scene? If the model capable of detecting multiple pedestrians in the scene? What is the processing time, if the processing time under a threshold for the purpose of using it in the real time scenario?
Which kind of algorithm to use used for second part of the problem, which deals with predicting the pedestrian intention? What is the time delay of such models if such model can be applicable in real time application scenarios?
How to choose right network for such problems, training them on large dataset which includes several thousand of images or hours of real video recordings? How to set and tune several hyper parameters while training the neural network? This Thesis work mainly answers above question by investigating and  applying several methodologies into practice.

\section{Objectives}
The main idea of this thesis is establishing a Pipeline by selecting appropriate models at both stages and evaluating the end results. Acquiring the public dataset related to pedestrian movement and validating the proposed model on them and estimating the performance in accordance with the dataset owner specification for doing so.

\section{Overview}
The texts in this thesis is split as below.
\begin{itemize}
  \item {\textbf {\textit{Chapter 2}} describes CNN for feature extraction, Deep Learning, state of the art}
  \item {\textbf {\textit{Chapter 3}} explains methodology choosing right algorithm}
	\item {\textbf {\textit{Chapter 4}} presents implementation, and shows the results of various experiments and setups  }
	\item {\textbf {\textit{Chapter 5}} concludes my thesis with a summary of the main results and brief discussion for probable future topics of research}
\end{itemize}

\section{Terminology}
In the current work and by other authors also training and learning is used interchangeably. Let me loosely define some frequently used terms and concepts in the Machine Learning arena.
\\*\textbf{Epoch:}
During the learning the network sees the set of samples several times. During the training, presenting the network entire set of sample once is known as an epoch. So an epoch represents one iteration over entire dataset.
\\*\textbf{Batch and batch size:}
Mainly because of two reasons we can not pass the entire dataset to the network for the training purpose at once. Firstly, datasets by nature most of the time are huge. Let it be image data or some other textual data, in many scenarios it is not possible to feed all the data because of hardware constraints, such as not enough RAM to hold entire dataset.
Secondly to update weight during training process, network has to wait for a very very long time to calculate the delta weight after processing all the input data. To solve this problem usually the full dataset is split into several small batches and number of samples within each small batch is known as 
Batch and batch size.
\\*\textbf{Iterations:} 
Number of batches that a neural network process to complete a single epoch.
The number of  iterations, batch size and  number of data samples in the dataset is given by, the below expression.
\begin{equation}
    D = B * I
\end{equation}
Where D is the total number of samples in the dataset.
\\*B is the number of samples in the mini batch that is fed to the network at once.
\\*I is the  total number of batches the network process in a single epoch.

Larger batch size requires more computational resource and achieves faster completion, in contrast smaller batch size leads to more generalization. In this regards, Yann  LeCun humorously said
\begin{quote}
``Training with large mini batches is bad for your health. More importantly, it's bad for your test error.  Friends, don't let friends use mini batches larger than 32.''
\end{quote}   
The empirical study of the performance of mini-batch stochastic gradient descent in \cite{masters2018revisiting} show that, the team obtained the best training stability and generalization performance using small batch sizes, for a given computational cost, across a wide range of experiments they conducted. In all cases they have achieved the best results with batch sizes m = 32 or smaller.


%optimizations are described in detail in section~\ref{ch1:opts}.

%\section{Description of micro-optimization}\label{ch1:opts}

%multiplies\footnote{Using unnormalized numbers for math is not a new idea; a

%unnormalized arithmetic, with a separate {\tt NORMALIZE} instruction.}.

% This is an example of how you would use tgrind to include an example
% of source code; it is commented out in this template since the code
% example file does not exist.  To use it, you need to remove the '%' on the
% beginning of the line, and insert your own information in the call.
%
%\tagrind[htbp]{code/pmn.s.tex}{Post Multiply Normalization}{opt:pmn}


\subsection{Block Exponent}

In a unoptimized sequence of additions, the sequence of operations is as
follows for each pair of numbers ($m_1$,$e_1$) and ($m_2$,$e_2$).
\begin{enumerate}
  \item Compare $e_1$ and $e_2$.
  \item Shift the mantissa associated with the smaller exponent $|e_1-e_2|$
        places to the right.
  \item Add $m_1$ and $m_2$.
  \item Find the first one in the resulting mantissa.
  \item Shift the resulting mantissa so that normalized
  \item Adjust the exponent accordingly.
\end{enumerate}


\begin{eqnarray*}
a_i & = & a_j + a_k \\
a_i & = & 2a_j + a_k \\
a_i & = & 4a_j + a_k \\
a_i & = & 8a_j + a_k \\
a_i & = & a_j - a_k \\
a_i & = & a_j \ll m \mbox{shift}
\end{eqnarray*}
instead of the multiplication.  For example, to multiply $s$ by 10 and store
the result in $r$, you could use:
\begin{eqnarray*}
r & = & 4s + s\\
r & = & r + r
\end{eqnarray*}
Or by 59:
\begin{eqnarray*}
t & = & 2s + s \\
r & = & 2t + s \\
r & = & 8r + t
\end{eqnarray*}

{\em might\/} 

Challenges are there while just considering image data, 
the verities of image sources for example we can get photos that are taken by 
professionals, synthetic photos drawn by image generators and real life photos 
that we see and capture in our day to day life. So the results of these benchmarks and 
observation across these aforementioned classes of data set does not transfer to the other scenarios.