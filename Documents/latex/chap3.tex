%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.

\chapter{Methodology}
This chapter describes the approaches that are planned and outlined before starting of the actual implementation. The primary goal of the thesis was to identify various stages of the pipeline which can be employed at prediction of pedestrian's future location. After the literature review i was motivated by the SSD for identification of pedestrian and locating with help of bounding box and usage of LSTM as \textit{future prediction} by considering pedestrian past move sequence as a time-series data. Also this chapter describes some of the key concepts and terminology used in the next chapter. These are some of the key concept in the area of machine learning with image data. Though the list of such concepts are exhaustive, the minimal terminologies and concepts are presented here.


\section{Bounding box prediction: RNN/LSTM}
Recurrent neural network primarily are densely connected neural network similar to feed forward network with a key difference; introduction of \textit{time} component in RNN. The output of a hidden layer in an RNN is \textit{fed back } into itself. With this we can model the data which is time dependent or sequence dependent in nature. In programming terms this can be represented by a fixed program taking certain inputs and some internal variable.

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[item/.style={circle,draw,thick,align=center},
itemc/.style={item,on chain,join}]
 \begin{scope}[start chain=going right,nodes=itemc,every
 join/.style={-latex,very thick},local bounding box=chain]
 \end{scope}
 \node[left=2em,item] (AL) {$A$};
 \path (AL.west) ++ (-1em,2em) coordinate (aux);
 \draw[very thick,-latex,rounded corners] (AL.east) -| ++ (1em,2em) -- (aux) 
 |- (AL.west);
 \draw[very thick,-latex] (AL.north) -- ++ (0,2em)
 node[above,item,fill=gray!10] {$h_t$};
 \draw[very thick,latex-] (AL.south) -- ++ (0,-2em)
 node[below,item,fill=gray!10] {$x_t$};
\end{tikzpicture}
\caption{Compact RNN loop}
\end{center}
\end{figure}

\newpara The above diagram depicts a compact loop form of an RNN showing a loop. An RNN can be thought as a multiple copies of the same network, each previous network passing a message to it successor. By unrolling the compact for we can visualize the RNN as below. 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[item/.style={circle,draw,thick,align=center},
itemc/.style={item,on chain,join}]
 \begin{scope}[start chain=going right,nodes=itemc,every
 join/.style={-latex,very thick},local bounding box=chain]
 \path node (A0) {$A$} node (A1) {$A$} node (A2) {$A$} node[xshift=2em] (At)
 {$A$};
 \end{scope}
 \node[left=1em of chain,scale=2] (eq) {$=$};
 \node[left=2em of eq,item] (AL) {$A$};
 \path (AL.west) ++ (-1em,2em) coordinate (aux);
 \draw[very thick,-latex,rounded corners] (AL.east) -| ++ (1em,2em) -- (aux) 
 |- (AL.west);
 \foreach \X in {0,1,2,t} 
 {\draw[very thick,-latex] (A\X.north) -- ++ (0,2em)
 node[above,item,fill=gray!10] (h\X) {$h_\X$};
 \draw[very thick,latex-] (A\X.south) -- ++ (0,-2em)
 node[below,item,fill=gray!10] (x\X) {$x_\X$};}
 \draw[white,line width=0.8ex] (AL.north) -- ++ (0,1.9em);
 \draw[very thick,-latex] (AL.north) -- ++ (0,2em)
 node[above,item,fill=gray!10] {$h_t$};
 \draw[very thick,latex-] (AL.south) -- ++ (0,-2em)
 node[below,item,fill=gray!10] {$x_t$};
 \path (x2) -- (xt) node[midway,scale=2,font=\bfseries] {\dots};
\end{tikzpicture}
\caption{Unrolling RNN loop}
\end{center}
\end{figure}
Mathematically RNNs can be represented as below: 

\begin{equation}
	\textbf{h}_t = f_W(\textbf{h}_{t-1} + \textbf{x}_{t}) 
\end{equation}

Where \textbf{\textit{ f}} is some function with parameter \textbf{W}. \\
This same function \textbf{\textit{ f}} and same weight matrix \textbf{W} is used at every step of the computation. \\
\textit{$h_t$} is new state \\
\textit{$h_{t-1}$} is old state \\
\textit{$x_t$} input vector at time step \textit{t}\\

In a more simplified form we can have the above function as:

\begin{equation}\label{hidden_state}
	\textbf{h}_t = \sigma (\textbf{Ux}_t + \textbf{Vh}_{t-1}) 
\end{equation}

Where \textit{U} is the input weight matrix and \textit{V} is the recurrent outputs. Time stamp denoted by \textit{t}. $\sigma$ represent an activation function e.g a \textit{tanh} or \textit{sigmoid}. If we unfold the above equation and go back three time step we have,

\begin{equation}
	\textbf{h}_t = \sigma (\textbf{Ux}_t + \textbf{V}(\sigma(\textbf{Ux}_{t-1} + \textbf{V}(\sigma(\textbf{Ux}_{t-2})))
\end{equation}

And in general, if we unfold in n time slot back,

\begin{equation}
\textbf{h}_t = \sigma (\textbf{Ux}_t + ...( \textbf{V}(\sigma(\textbf{Ux}_{t-n+2} + \textbf{V}(\sigma(\textbf{Ux}_{t-n+1})..)))
\end{equation}

\begin{equation}
	\textbf{y}_t = (\textbf{W}_{hy}  \textbf{h}_{t}) 
\end{equation}

Where \textbf{$y_t$}  represents output at a time stamp \textit{t} \\
\textbf{$h_t$} is the hidden state, computed as in equation \ref{hidden_state}

The problem in general suffered by a plain RNN is over a large period of time the back propagation gradients either exploding or vanishing. During a \textit{vanishing gradient } problem, the gradients of the network output with respect to the early layer of the network becomes extremely small, which indicates a large change in the parameters for the early layers does not have a big effect on network output. Usually this happens when the activation function such as \textit{sigmoid} or \textit{tanh} squash their input into a small output range non-linearly. Considering the activation function sigmoid which is defined as below,

\begin{equation}
	S(x) = \frac{1}{ 1+ e^x}
\end{equation}

Sigmoid is a special case of standard Logistic function defined as 
\begin{equation}
f(x) = \frac{L}{ 1+ e^{-k(x-x_0)}}
\end{equation}
Where \\
L: Curve's maximum value \\
k: Steepness of the curve \\
(x0 - x): value of Sigmoid's midpoint \\

A standard logistic function is called sigmoid when (k=1,x0=0,L=1) and can be viewed as below: \\
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\begin{axis}%
			[
			grid=major,     
			xmin=-10,
			xmax=10,
			axis x line=bottom,
			ytick={0,.5,1},
			ymax=1,
			axis y line=middle,
			]
			\addplot%
			[
			blue,%
			mark=none,
			samples=200,
			domain=-6:6,
			]
			(x,{1/(1+exp(-x))});
		\end{axis}
	\end{tikzpicture}
	\caption{Sigmoid function}
	\label{fig:Sigmoid-function}
\end{figure}

Sigmoid function can be re written as  \\
\begin{equation}
	S(x) = (1+e^{-x})^{-1}
\end{equation}
And by taking the derivative \\
\begin{equation}
\frac{d}{dx}S(x) = \frac{d}{dx} (1+e^{-x})^{-1} \\
= \frac{e^{-x}} {(1+e^{-x})^{-2}} 
\end{equation}

Plotting the derivative of the sigmoid activation function as below, we notice that 
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}
\begin{axis}%
[
grid=major,     
xmin=-10,
xmax=10,
axis x line=bottom,
ytick={0,.5,1},
ymax=1,
axis y line=middle,
]
\addplot%
[
blue,%
mark=none,
samples=100,
domain=-10:10,
]
(x,{exp(-x)/((1+exp(-x)) * (1+exp(-x)))});
\end{axis}
\end{tikzpicture}
\caption{Derivative of the sigmoid activation function}
\end{center}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\begin{axis}%
			[
			grid=major,     
			xmin=-10,
			xmax=10,
			axis x line=bottom,
			ytick={0,.5,1},
			ymax=1,
			axis y line=middle,
			]
			\addplot%
			[
			blue,%
			mark=none,
			samples=200,
			domain=-6:6,
			]
			(x,{(exp(2*x)-1 )/(exp(2*x)+1 )});
		\end{axis}
	\end{tikzpicture}
	\caption{Hyperbolic Tangent Function}
	\label{fig:tanh-function}
\end{figure}

the sigmoid derivative is well below the 1.0 in the whole range of input values. And this makes the gradients vanishing quite fast as, multiplying this value \footnote{value less than 1} several times for several layer \textit{ back propagation} brings the value close to 0.


\subsection{LSTM}
LSTM has a chain like structure as in RNN but the repeating module has a different structure. There are four neural network layer as contrast with RNN that has a single neural network layer. The four neural network layers interact in a very special way. LSTM networks have gained acceptance for time sequence data processing, such as image captioning, action recognition, speech recognition, language translation etc. An LSTM visually can be shown as below. 

\begin{figure}[H]
\begin{align*}
\begin{tikzpicture}[baseline=-1pt]
\node[] at (0,0) {\includegraphics[scale=0.8]{LSTM3-chain}};
\end{tikzpicture}&
\end{align*}
\begin{center}
\caption{LSTM schematic diagram}
\end{center}
\end{figure} 

The core idea to LSTM is the cell state, it run through the entire chain and performs some minor linear interaction. LSTM adds or removes information to the cell state with the help of regulated structure called gates. Gates are represented by a neural network layer sigmoid activation and a point-wise multiplication operation.
A sigmoid outputs how much of a component should be let through. A value of 0 blocks the whole component and a value of 1 let entire component through. In a basic LSTM, there are four such gates.
With arrival of new information, the model first decides which long term information is not needed anymore, and forgets it. Stores necessary part of the new information into long-term memory. An LSTM has three such gates, that controls the cell state.
A \textbf{remember gate} is learned that takes new input and working memory as input and outputs \textit{n} bummers between 0 and 1, each determines degree to which long-term memory element to be kept. Where 1 indicates to keep it and 0 indicates to forget it entirely. A small neural network can model this \textbf{remember gate} given below. In the literature this is also known as \textbf{forget gate}. Below mathematical expressions details various internal operation within a LSTM memory cell as discussed in original paper of Hochreiter et.al \cite{hochreiter1997long} and simplified rewriting \cite{christopherolah}

\begin{equation}
f_t = \sigma(W_f. [h_{t-1}, x_t  ] + b_f)
\end{equation}
Where $f_t$ is forget gate's activation vector \\
$W_f$ is the weight matrix for the forget gate \\
$h_{t-1}$ is the cell state at time (t-1) \\
$b_f$ is the bias for the forget gate

After forgetting the unnecessary information, LSTM is interested in storing the new required information in the cell state. This involves two step, with a \textbf{input} sigmoid layer values that needs to be updated is decided. And then a tanh layer creates a vector for new candidate values for cell state that could be added to the cell state. 

\begin{equation}
i_t = \sigma(W_i. [h_{t-1}, x_t  ] + b_i)
\end{equation}
Where $i_t$ is input gate's activation vector 
$W_i$ is the weight matrix for the input gate \\
$h_{t-1}$ is the hidden state at time (t-1) \\
$b_i$ is the bias for the input gate

\begin{equation}
C_t\textasciitilde = tanh(W_C. [h_{t-1}, x_t ] + b_C)
\end{equation}
$C_t\textasciitilde$ represents vector of new candidate values for the cell state
$W_C$ is the weight matrix for the tanh layer that computes the candidates\\
Both $i_t$ and $C_t\textasciitilde$ is combined to update the cell state as below.

\begin{equation}
C_t = f_t * C_{t-1} + i_t * C_t\textasciitilde
\end{equation}

Next, output is decided based on cell state and a output gate, that decides what part of the cell state are going to output. Mathematically this can be represented as below.

\begin{equation}
o_t = \sigma(W_o. [h_{t-1}, x_t  ] + b_o)
\end{equation}
\begin{equation}
h_t = o_t  * tanh(C_t)
\end{equation}
Where $o_t$ is output gate's activation vector 
$W_o$ is the weight matrix for the output gate \\
$h_{t-1}$ is the hidden state at time (t-1) \\
$b_o$ is the bias for the output gate

There are several variants of LSTM, but basic variant was explored and used in the scope of the thesis.

\subsection{LSTM Cell refinement} \label{state_refinement}

The position of the pedestrian bounding box largely depends upon speed of the vehicle, pedestrian own speed and the direction pedestrian makes with the camera and other social behavior such neighboring pedestrians, stationary obstacles etc. We propose an obvious factor which is not yet considered while predicting the future trajectory of the pedestrian by taking an important parameter into consideration; that pedestrian and vehicle always want to remain at safe distance from each other. A car lowers its speed greatly when moving at high speed and finds a pedestrian near the curb waiting to cross or crossing the road. With a lesser speed and within the safe distance from pedestrian, it keeps moving slowly without halting even though pedestrians are moving in front of it.
It is also noticed, as pedestrian moves with natural speed when a vehicle is not near and he feels confident that he is in safe zone, in contrast he increases his speed when vehicle approaches. So there exists a social relationship between vehicle and pedestrian and we attempted at extracting this information and make refinement to the LSTM state. With the motivation from \cite{zhang2019sr}, we constructed a message passing mechanism to refine the features of pedestrian by the current vehicle speed and the direction between pedestrian and vehicle(camera mounted on the vehicle). The SR module takes vehicle current speed, pedestrian movement information with respect to vehicle, cell states and hidden states from the LSTM as input and outputs the refined cell state. This can be expressed as 
%
\begin{equation}
\hat{C}^{t, l+1}= M(V^t, {h}^{t, l}) + \hat{C}^{t, l}
\end{equation}
Where $\hat{C}^{t, l+1}$ represents refined cell state at time stamp t and refined iteration l, $V^t$ is the vehicle speed at time t and rate of pedestrian location change in X-Y \\
${h}^{t, l}$ is the hidden state of the Pedestrian at time stamp t

After L iteration of refinement in the SR module the updated equations would be

\begin{equation}
\begin{array}{l@{}}
\hat{C}^{t}=\hat{C}^{t, L} \\
\hat{h}^{t}={g}^{o,t} \odot tanh(\hat{C}^{t}) \\
\left[ \hat{x}^{t+1}, \hat{y}^{t+1}, \hat{w}^{t+1}, \hat{h}^{t+1} \right] ^ T = W_p\hat{h}^{t}
\end{array}
\end{equation}

$\hat{w}^{t+1}$, $\hat{h}^{t+1}$ represents the bounding box width and height.
$W_p$ is the weight matrix of the output gate, and \textit{p} symbolizes this is modeled for pedestrian
%
In this case the message passing can be formulated as below 
\begin{equation}
\hat{C}^{t, l+1}=M(P_t, V_t) +  \hat{C}^{t, l}
%{W}^{vp}\alpha_v
\end{equation}
$\hat{C}^{t, l+1}$ is the refined cell state for (l+1)th iteration at the t-time stamp \\
$M(P_t, V_t)$ is the message passing between state of pedestrian and vehicle as described below.

We propose the message passing term $M(P_t, V_t)$ using awareness gate as below.
\begin{equation}
M(P_t, V_t) = W^{a,p}.(g^{a,t,l} \odot {h}^{t, l} ) 
%+ \hat{C}^{t, l}
\end{equation}
Where $W^{a,p}$ is a linear transform parameter and $g^{a,t,l}$ is the awareness gate vector and formulated by

\begin{equation}
g^{a,t,l} = \sigma(W^{a} [ \alpha_p, {h}^{t, l} ] + b^{a} ) 
%+ \hat{C}^{t, l}
\end{equation}

Where $W^{a}$, $b^{a}$ are parameters and $\sigma$ denotes the sigmoid function, $\alpha_p$ is the awareness factor of the pedestrian and described below \ref{awareness_factor}.
$g^{a,t,l}$ selects features from ${h}^{t, l}$ by element-wise product.

So 
\begin{equation}
\hat{C}^{t, l+1}= W^{a,p}.(g^{a,t,l} \odot {h}^{t, l} )+  \hat{C}^{t, l}
%{W}^{vp}\alpha_v
\end{equation}

%Where\alpha_v is the awareness factor of the vehicle as described below, ${W}^{vp}$ is a linear transformation used for the transmission of message from vehicle to pedestrian.
%Where $V_v$ is the velocity (speed and direction) of the vehicle and ${W}^{vp}$ is a linear transformation used for the transmission of message from vehicle to pedestrian.
%Consider a person represented as a bounding box in a frame by (x,y,w,h), where (x,y) is the top left corner co-ordinate and \textit{w} is the width of the bounding box and \textit{h} is the height of the bounding box. The position of the the pedestrian bounding box in the next frame is given by the below relation

%\begin{equation}
%\begin{array}{l@{}}
%{P}_{t}=(x_t,y_t,w_t,h_t) \\
%{P}_{\hat{t}}=(x_{\hat{t}},y_{\hat{t}},w_{\hat{t}},h_{\hat{t}})
%\end{array}
%\end{equation}

%Where ${P}_{\hat{t}}$ is the position of the pedestrian at time stamp $(t+\Delta t)$, and $\Delta t$ is given by below equation.

%where $\hat{t}$ is time stamp at which the next frame is captured and can be given by below expression
%\begin{equation}
%\begin{array}{l@{}}
%\hat{t} = t +  \Delta t \\
%\Delta t = \frac{1}{fps}
%\end{array}
%\end{equation}

%Where \textit{fps} is a measure of how many frames captured by the camera in one second

%$(x_{\hat{t}},y_{\hat{t}},w_{\hat{t}},h_{\hat{t}})$ in the equation depends on basically three factors speed of the vehicle, speed of the pedestrian and the angle between vehicle camera and pedestrian. Also it is necessary to be noted that other social aspect such as obstacle, neighborhood pedestrian, sudden change in pedestrian's own decision for the destination and displacement of the camera will have impact.

%A vehicle moving with a speed of $V_c$ shall travel a distance within \textit{a frame }time duration is given by the below equation.

%\begin{equation}
%d_v = \Delta t \times V_c
%\end{equation}

%A person moving with a speed of $V_p$ shall travel a distance within \textit{a frame }time duration is given by the below equation.

%\begin{equation}
%d_p = \Delta t \times V_p
%\end{equation}

%The new coordinates $(\hat{x}, \hat{y})$ for the pedestrian is given by

%\begin{equation}
%\begin{array}{l@{}}
%\hat{x} =x + d_p cos\theta \\
%\hat{y} =x + d_p sin\theta
%\end{array}			
%\end{equation}

%Due to this change of distance $\Delta d_v$ and $\Delta d_p$, the resultant distance and angle between vehicle and pedestrian shall be changed and given by below relation. Considering the car camera coordinate in the previous frame at the origin, it moves always in the direction of \textit{y-axis}, its new co-ordinate at $\hat{t}$ shall be $(x_0, \Delta d)$

%\begin{equation}
%\begin{array}{l@{}}
%d =\sqrt{(x_0 - x)^2 + (d_v - y)^2} \\
%	=\sqrt{(x)^2 + (d_v - y)^2} \\
%\hat{\theta }= tan^-1\frac{(y-d_v)}{(x-x_0)} \\
%\end{array}
%\end{equation}

%\begin{equation}
%\begin{array}{l@{}}
%\theta =\hat{\theta } + {\theta }_f\\
%\end{array}			
%\end{equation}

%Where ${\theta }_f$ is the focal angle made by change in vehicle angle.

%The distance between camera and object in the scene can be derived by below equation.
%\begin{equation}
%d = \frac{w \times f}{p}
%\end{equation}
%where d is the distance between camera and object, w corresponds to width of the object, f represents the focal length of the camera and p is the perceived width in number of pixel in the image.
%So with a moving camera, modified perceived width in pixel given by

%\begin{equation}
%\hat{p} = \frac{w \times f}{d_{\hat{t}}}
%\end{equation}
%Where $d_{\hat{t}}$ is the distance between camera and object at $\hat{t}$.

\newpara
\textbf{Awareness:}
Considering ${t_p}$ represents vehicle time to reach pedestrian and \\
${t_c}$ is the time pedestrian needs to cross the road and reaches a safer location,
${t_p}$ and ${t_v}$  can be calculated as below.

\begin{equation}
\begin{array}{l@{}}
  t_p = \frac{d_p}{s_v} \\
	t_c = \frac{d_c}{s_p}
\end{array}
\end{equation}

Where ${d_p},{s_v}$ represent distance between vehicle and pedestrian and speed of the vehicle\\
${d_c}, {s_p}$ represent distance between pedestrian and safe destination (eg. other side of the road)and speed of the pedestrian respectively \\
Awareness factor of pedestrian ${\alpha_p}$ and vehicle ${\alpha_v}$, 
which are related by below relation for a safe driving scenario and they are dependent on ${t_c}$ and ${t_c}$ in a certain way is proposed.

\comment
{
}
\begin{equation} \label{awareness_factor}
  \alpha_p + \alpha_v = 1
\end{equation}

And $\alpha_p$ is given by a linear function \cite{osaragi2004modeling} $f(t_p, t_c)$ 

Now let's consider three scenarios:

\begin{itemize}
\item 
For ${t_p}>>{t_c}$ vehicle is at far away and both pedestrian and vehicle can just maintain their current speed. $\alpha_p$ = $\gamma,$ where $\gamma$ is considered to be fairly safe value for the system

\item 
For ${t_c}>>{t_p}$ this is undesirable and is a catastrophic scenario and not expected if the system is operational. $\alpha_p = 0$

\item
	Now considering some positive value $\beta_1$ and $\beta_2$ which gives a lower bound and upper bound of the ratio between $t_p$ and $t_c$. We are interested in evaluating the dynamic of \textit{awareness} within this the value range ($t_p, t_c$). $\beta_1$ and $\beta_2$ can be mathematically shown as below. More on such parameter can be found in \cite{yousef2016forward}.

	\begin{equation}
	\begin{array}{l@{}}
		\beta_1 \times {t_c} < {t_p} \\
		\beta_2 \times {t_c} > {t_p}
	\end{array}
	\end{equation}

	$f(t_p, t_c)$ is defined as follows, where $\hat{t}$ is given by $\frac{t_p}{t_c}$\\
	\begin{empheq}[left=\empheqlbrace]{align}
		0, \hat{t} < \beta_1 \\
		\gamma, \hat{t} > \beta_2 \\
		\gamma \times \frac{\hat{t}}{\beta_2 - \beta_1}, \beta_1 > \hat{t} > \beta_1
	\end{empheq}

\end {itemize}

Considering the pedestrian and vehicle system always expected to be safe, LSTM cell state may be refined with calculated value for $\alpha_v$

\newpara
\textbf{Pedestrian bounding box location calculation:}
To aid the LSTM cell refinement because of vehicle movement and its change in direction we proposed the following. Assuming the vehicle from a constant distance from a virtual point on the x-axis in the image frame of reference.

Considering the below representation, a person represented as a bounding box in a frame by (x,y,w,h), where (x,y) is the top left corner co-ordinate and \textit{w} is the width of the bounding box and \textit{h} is the height of the bounding box. The position of the the pedestrian bounding box in the next frame is symbolically shown as ${P}_{\hat{t}}$.

\begin{equation}
\begin{array}{l@{}}
{P}_{t}=(x_t,y_t,w_t,h_t) \\
{P}_{\hat{t}}=(x_{\hat{t}},y_{\hat{t}},w_{\hat{t}},h_{\hat{t}})
\end{array}
\end{equation}

Where ${P}_{\hat{t}}$ is the position of the pedestrian at time stamp $(t+\Delta t)$, and $\Delta t$ is given by below equation.

where $\hat{t}$ is time stamp at which the next frame is captured and can be given by below expression
\begin{equation} \label{delta-t}
\begin{array}{l@{}}
\hat{t} = t +  \Delta t \\
\Delta t = \frac{1}{fps}
\end{array}
\end{equation}


Where \textit{fps} is a measure of how many frames captured by the camera in one second

$(x_{\hat{t}},y_{\hat{t}},w_{\hat{t}},h_{\hat{t}})$ in the equation depends on basically three factors speed of the vehicle, speed of the pedestrian and the angle between vehicle camera and pedestrian. Also it is necessary to be noted that other social aspect such as obstacle, neighborhood pedestrian, sudden change in pedestrian's own decision for the destination and displacement of the camera will have impact.
Here we focused the aspect of vehicle's motion (speed and direction).


%A vehicle moving with a speed of $V_c$ shall travel a distance within \textit{a frame }time duration is given by the below equation.

%\begin{equation}
%d_v = \Delta t \times V_c
%\end{equation}

%A person moving with a speed of $V_p$ shall travel a distance within \textit{a frame }time duration is given by the below equation.

%\begin{equation}
%d_p = \Delta t \times V_p
%\end{equation}

%The new coordinates $(\hat{x}, \hat{y})$ for the pedestrian in the image frame of reference is given by

%\begin{equation}
%\begin{array}{l@{}}
%x_{\hat{t}} =x_t + \frac{d_p cos\theta }{CF}\\
%y_{\hat{t}} =y_t + \frac{d_p sin\theta }{CF}
%\end{array}			
%\end{equation}

%Where $\theta$ is the angle of the pedestrian movement \\
%CF is the calibration factor of the camera and used for scale conversion.

%Assuming the initial position of the pedestrain (x,y) on the ground frame-of-reference and due to this change of distance $\Delta d_v$ and $\Delta d_p$, the resultant distance and angle between vehicle and pedestrian shall be changed and given by below relation. Considering the car camera coordinate in the previous frame at the origin, it moves always in the direction of \textit{y-axis}, its new co-ordinate at $\hat{t}$ shall be $(x_0, d_v)$. New position of the pedestrian shall be $(x + d_p cos\theta, y + d_p sin\theta)$

%Assuming the initial position of the pedestrian (x,y) on the ground frame-of-reference and due to this change of distance $\Delta d_v$ and $\Delta d_p$, the resultant distance between vehicle and pedestrian shall be changed and given by below relation. Considering the car camera coordinate in the previous frame at the origin, it moves always in the direction of \textit{y-axis}, its new co-ordinate at $\hat{t}$ shall be $(x_0, d_v)$. New position of the pedestrian shall be $(x + d_p cos\theta, y + d_p sin\theta)$


%\begin{equation}
%\begin{array}{l@{}}
%d_{\hat{t }} =\sqrt{(x_0 - x-d_p cos\theta)^2 + (d_v - y-d_p sin\theta)^2} \\
%\hat{\theta}= tan^-1\frac{(y-d_v)}{(x-x_0)} \\
%\end{array}
%\end{equation}


Consider the mid point ($P$) of the lower side of the pedestrian bounding box as the initial position of the pedestrian (x,y) on the image frame-of-reference.

%, the initial angle between camera and pedestrain can be approximated as

%\begin{equation}
%{\theta }= tan^-1\frac{(y-y_0)}{(x-x_0)}
%\end{equation}

%\textit{Note:} Some image processing software package considers top-left corner as (0,0) and some others consider bottom-left corner as (0,0). For the simplicity purpose we assumed  bottom-left corner as (0,0).

Using the given equation in \cite{dougan2010real}, which tries to estimate instantaneous velocity vector of a point (in 2D space) using displacement vector which depend upon scale of the image, frame rate and vehicle speed.
\begin{equation} \label{conversion}
{v}= \frac{\delta p}{\delta t}
\end{equation} 
v is the instantaneous velocity vector \\
${\delta p}$ displacement vector, displacement vector measured in pixel units \\
${\delta t}$ is the frame capture rate

The magnitude of this vector in image space is transformed to object space using scale factor as described below. Scale factor (SF) related to distance between camera and object and the focal length of the camera.

Object space displacement can be computed as below

\begin{equation} \label{conversion}
{d_o}= {d_i} \times SF
\end{equation} 

where $d_o$ represents the distance in object space \\
$d_i$ represents the displacement in image space \\
SF is the scale factor

When the vehicle moves with an angle ${\theta }$ in a time period of ${\Delta t}$, the point P now displaced by the same angle as ${\theta}$. Consider the vehicle speed \textit{v}, the displacement of the point \textit{P} on the image is given by

\begin{equation} \label{distance-in-pixel}
{d_1}= \frac{v \times \Delta t}{SF}
\end{equation} 

Where ${d_1}$ is the displacement in pixels, SF is the scale factor of the camera that converts real world distance to distance in pixels, $\Delta t$ as given in \ref{delta-t}.

Now to determine this translated point ${P_1}$, laws of sine is used as given below. 

\begin{equation}
\frac{d_1}{sin \theta_1} = \frac{d}{sin \theta_2} 
\end{equation}

In the above equation, $\theta_1$ is the angle made by vehicle, $\theta_2$ is the only unknown, after finding that $\theta_3$ can be easily found.

By applying same laws of sine, the distance $d_c$ (from the imaginary vehicle point to the point $P_1$) in pixels can be evaluated from below equation.

\begin{equation} \label{camera-distance}
\frac{d_c}{sin \theta_3} = \frac{d}{sin \theta_2} 
\end{equation} 


The resultant angle between vehicle and pedestrian, ${\hat{\theta}}$ can be expressed as below.
\begin{equation} \label{theta-large}
\begin{array}{l@{}}
\hat{\theta } = \theta  + {\theta}_1
\end{array}			
\end{equation} 

Where ${\theta}$ is the initial angle between vehicle and pedestrian and ${{\theta}_1}$ is the angle made by vehicle.

The coordinates of point ${P_1(x_1, y_1)}$ can be formulated as
\begin{equation}
\begin{array}{l@{}} 
x_1 = d_c cos \hat{\theta } \\
y_1 = d_c sin \hat{\theta } 
\end{array}			
\end{equation} 
%\label{new-xy}

Where $d_c$ is given as in equation \ref{camera-distance} and $\hat{\theta}$ in \ref{theta-large}

Determination of the distance to an object \cite{jungel2007improving} can be determined from size of the object and the focal length of the camera are known.
The distance between camera and object in the scene can be derived by below equation.

\begin{equation} \label{distance-camera-object}
d_object = \frac{w \times f}{p}
\end{equation} 

where \textit{d} is the distance between camera and object, \textit{w} corresponds to width of the object, \textit{f} represents the focal length of the camera and \textit{p} is the perceived width in number of pixel in the image.

So with a moving camera, modified perceived width and height in pixel represented as ${p_w}$ ${p_h}$ respectively given by below expression where $w_s$ and $h_s$ are the standard width and height of a pedestrian and used as a constant.

\begin{equation} \label{new-height-width-pixels}
\begin{array}{l@{}}
{p_w} = \frac{w_s \times f}{d_{\hat{t}}} \\
{p_h} = \frac{h_s \times f}{d_{\hat{t}}}
\end{array}			
\end{equation} 

Where ${d_{\hat{t}}}$ is the distance between camera and object at ${\hat{t}}$ given by below expression

\begin{equation}
d = d_p \times {CF} + k
\end{equation}
where $d_p$ is the measured pixel distance as in \ref{distance-in-pixel}, k is a constant, its the least distance between the camera and the point in the ground that is captured in the image frame.

Combining the results of eq \ref{new-height-width-pixels} and \ref{new-height-width-pixels}, bounding box for the pedestrian is calculated as $({x_1}, {y_1},{p_w}, {p_h})$

%\comment{
%}

\textbf{Cell state Vs. Hidden State:}
Two entities such as \textit{cell state} and \textit{hidden state} are carried over from one time stamp (One LSTM Cell) to another (next LSTM Cell) in an LSTM. Typically they can be represented by vectors. In an LSTM Cell, \textit{hidden state }contains information about previous inputs. The hidden state from the previous LSTM Cell and current input are combined and processed through various gates in a particular LSTM Cell. Cell state can be regarded as the memory of the LSTM Cell. There are several neural network behaves as gates and they act together to decide which information are allowed to be part of \textit{cell state}. Output-gate output and the updated cell state decide the next hidden state. \textit{Hidden states} are the output of an LSTM Cell or layer of cells and they are used for predictions.  

\section{Object detection: SSD}
After image data acquisition and data preparation, the challenging task remains, how to train the model. 
To better understand the SSD framework and training steps, a flowchart is prepared that detailed it's workings. In the current work and by other authors also the term \textit{training }and \textit{learning}is used interchangeably. 

\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[
    node distance = 8mm and 20mm,
      start chain = going below,
     arrow/.style = {thick,-stealth},
      base/.style = {% common features of all nodes
            draw, thick, 
            minimum width=30mm, minimum height=10mm, align=center,
            inner sep=1mm, outer sep=0mm,
            on chain, join=by arrow},
  decision/.style = {diamond, base,
            aspect=1.5, inner xsep=0mm},
   process/.style = {rectangle, base},
 startstop/.style = {rectangle, rounded corners, base},
 stop/.style = {rectangle, rounded corners},
                        ]
\node (start) [startstop] {Start};
\node (0) [process]{Image data is prepared with data augmentation}; 
\node (1) [process]{A set of default bounding boxes associated with each feature map cell}; 
\node (2) [process]{ At each feature map cell, offsets relative to default box shapes in the cell is predicted}; 
\node (3) [process]{ Per-class score that indicates class instance is predicted}; 
\node (4) [process]{ Each ground truth box is matched with the default box with best jaccard overlap};
\node (5) [process]{ Match default box to any ground truth with jaccard overlap greater than 0.5}; 
\node (6) [process]{ A combined loss function and back propagation applied end-to-end}; 
\node (sevralepocs) [process]{ Train it for several epochs}; 
\node (7) [process]{ Validate and save the neural network model};
\node [decision] (validation) { Is validation error accepted?};
\node (end)  [startstop, below=10mm of validation]  {END};

\draw [arrow] (validation.west) to node [] {No} + (-3,0) |-  (sevralepocs.west);
\path (validation) to node []{Yes} (end);

\end{tikzpicture}
\begin{center}
\caption{Flowchart for SSD network training}
\end{center}
\end{center}
\end{figure}

After the model is trained, its accuracy and performance needs be measured and that is described in detailed in the subsequent chapter.

\subsection{Network architectures}
Below depicted the details of convolution in image and CNN building blocks.

\begin{figure}[H]
\begin{align*}
\begin{tikzpicture}[baseline=-3pt]
\node[] at (0,0) {\includegraphics[scale=0.5]{LeNet-Arch}};
\end{tikzpicture}&
\end{align*}
\begin{center}
\caption[Architecture of a traditional convolutional neural network.]{The architecture of the original convolutional neural network, as introduced by LeCun et al. (1989), alternates between convolutional layers including hyperbolic tangent non-linearities and subsampling layers. In this illustration, the convolutional layers already include non-linearities and, thus, a convolutional layer actually represents two layers. The feature maps of the final subsampling layer are then fed into the actual classifier consisting of an arbitrary number of fully connected layers. The output layer usually uses softmax activation functions. \footnote{http://alexlenail.me/NN-SVG/LeNet.html}}
	\label{fig:traditional-convolutional-network}
\end{center}
\end{figure}

\begin{itemize}
\item
\textbf{First Layer:} A convolution layer with depth 8, height 128, width 128 and filter size(8,8)
\item
\textbf{Second Layer:} Max-Pool layer
\item
\textbf{Third Layer:} A convolution layer with depth 8, height 64, width 64 and filter size(16,16)
\item
\textbf{Fourth Layer:} Max-Pool layer
\item
\textbf{Fifth Layer:} A convolution layer with depth 24, height 16, width 16 and filter size(8,8)
\item
\textbf{Fifth Layer:} A convolution layer with depth 24, height 16, width 16 and filter size(8,8)
\item
\textbf{Sixth and seventh Layer:} Are dense layers
\end{itemize}

\newpara \textbf{Convolution layer} It is the first layer to which, the input image is given. A small matrix otherwise known as a filter or kernel moves along the image and is convoluted with the input image. As a process of convolution \ref{fig:Schematic-representation} kernel elements are multiplied with the original pixel value and these values are summed up and produces a feature map as input for further layers.
 
\newpara \textbf{Nonlinear layer}
A non-linearity is added to the output generated from a convolution layer in the form of an activation function in this layer. The basic idea of adding a non-linear layer is to make different layers within the network have a non-linear relationship and this ensures the network can make more complex decisions.
Usually ReLU is used as a non-linear activation function and mathematically given as below.
%https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/

\begin{equation}
	f(x) =max(0, x)
\end {equation}
Where x is the input to a neuron and output is similar to ramp function.
Although simple, it has been demonstrated that this activation function enables better performance in deeper networks compared to widely used complex function such as \textit{sigmoid} (fig:\ref{fig:Sigmoid-function}) and \textit{hyperbolic tangent} (fig: \ref{fig:tanh-function}).

\newpara \textbf{Pooling layer}
The subsequent layer after non-linear layer. It down samples the image, as a result the processed image volume is reduced. Max-pooling is similar to convolution, but it chooses the maximum element within the area where the maxpool filter is applied. For a pooling size (2,2) the input is halved in both spatial dimensions.

\newpara \textbf{Fully connected layer}
Normally a fully connected layer is attached to the Network as last layer of the Network. Fully connected layer performs classification by taking input as features extracted and transformed in the previous conv-relu-pooing layers. This results in an N dimensional vector, where N is the number of classes the model is trained on.

\newpara CNN learns the filters that traditional image classification algorithm hand crafts. This makes CNN little pre-processing compared to other traditional algorithm and results superior results. There are many evolution of CNNs starting from LeNet (LeCun et al., 1989) for zip code recognition, LeNet-5 (1998) a 7 layer CNN \ref{fig:traditional-convolutional-network} by LeCun et al. for hand written numbers on the check recognition, AlexNet by Alex Krizhevsky et al. in 2012 used deeper layers and more filters per layer than leNet-5. It also used ReLU activation, max pooling, dropout, data agumentation and SGD with momentum during training. \footnote{These terms shall be discussed in the subsequent chapter.} This achieved a record breaking result at that time. ZFNet in 2013 winner of ILSVRC 2013 with a small tweaking to AlexNet. 2014, GoogleNet otherwise known as Inception V1 from Google achieved human level performance. Another net in the same year got the attention known as VGGNet. It uses only 3x3 convolution, 16 layers and with lots of filters. It consists of 138 million parameters and one of major disadvantage and required more time to train. And weight configuration of this is publicly available and used as baseline feature extractor.

%Standard CNN architectures, over the last 2 decades uses set of standard layer that can be graphically \ref{tomepel} represented as below. 

%\begin{figure}[H]
%\begin{align*}
%\begin{tikzpicture}[baseline=-3pt]
%\node[] at (0,0) {\includegraphics[scale=0.5]{input_layer}};
%\end{tikzpicture}&=
%
%\begin{tikzpicture}[baseline=-0pt]
%\draw (0,-1) rectangle (0+0.3,1);
%\node[align=center,scale = 0.65] at (0+0.15,0) {I \\ n \\ p \\ u \\ t};
%\end{tikzpicture}\;,&
%
%\begin{tikzpicture}[baseline=-3pt]
%\node[] at (0,0) {\includegraphics[scale=0.5]{VGG-conv}};
%\end{tikzpicture}&=
%
%\begin{tikzpicture}[baseline=-0pt]
%\filldraw[fill=blue!30!white] (0,-1) rectangle (0+0.3,1);
%\node[align=center,scale = 0.65] at (0+0.15,0) {C \\ o \\ n \\ v};
%\end{tikzpicture}\;,\notag\\
%
%\begin{tikzpicture}[baseline=-3pt]
%\node[] at (0,0) {\includegraphics[scale=0.5]{VGG-pool}};
%\end{tikzpicture}&=
%
%\begin{tikzpicture}[baseline=-0pt]
%\filldraw[fill=red!30!white] (0,-1) rectangle (0+0.3,1);
%\node[align=center,scale = 0.65] at (0+0.15,0) {P \\ o \\ o \\ l};
%\end{tikzpicture}\;,&
%
%\begin{tikzpicture}[baseline=-3pt]
%\node[] at (0,0) {\includegraphics[scale=0.5]{VGG-fc}};
%\end{tikzpicture}&=
%
%\begin{tikzpicture}[baseline=-0pt]
%\filldraw[fill=green!30!white] (0,-1) rectangle (0+0.3,1);
%\node[align=center,scale = 0.65] at (0+0.15,0) {F \\ u \\ l \\ l};
%\end{tikzpicture}
%\end{align*}
%\begin{center}
%\caption{CNN layers: Schematic representation, source:Thomas Epelbaum GitHub}
%\label{fig:Schematic-representation}
%\end{center}
%\end{figure}

%\newpara Based on these schematic layers, a simple graphical \footnote{\label{tomepel} Images source: Thomas Epelbaum GitHub: https:\textbackslash github.com\textbackslash tomepel\textbackslash Technical\_Book\_DLaccessed on 07th August 2019 } representation of VGG-Net is shown below.
%\begin{figure}[H]
%\begin{center}
%\begin{tikzpicture}
%\node[] at (0,0) {\includegraphics[scale=1]{VGG}};
%\end{tikzpicture}
%\caption{VGG-Net, souece: Thomas Epelbaum GitHub}
%\end{center}
%\end{figure} 

\section{Terminology}
Some of the key concepts that are studied and useful are presented in subsequent texts.

\subsection{Data Augmentation}
Data augmentation is a key in image based deep leaning application. This make the model robust to different object sizes, logically possible  additional training image generation via application of various image modification such as flipping horizontally \footnote{Many time flipping image vertically does not make sense in real world scenarios, wherever it makes sense, it can be done as well.}, shear, distortion, change in color intensity. In \cite{liu2016ssd} authors mentioned application of flipping each image randomly horizontally with probability of 0.5, this ensures objects appear on left and right with similar likelihood.

\subsection{Non-Maximum Suppression (NMS)}
During inference time, SSD predicts, a large number of boxes in a cluttered manner. To prune most of them, a technique called \textit{non-maximum suppression} is applied. This technique discards those boxes having label confidence less than a threshold and IoU less than a defined threshold. \footnote{Usually 0.45 is used as IoU threshold}. After discarding such boxes, only top \textit{N} predictions are returned which ensures most likely predictions.
\subsection{Epoch:}
During the learning the network sees the set of samples several times. During the training, presenting the network entire set of sample once is known as an epoch. So an epoch represents one iteration over entire dataset.
\subsection{Batch and batch size:}
Mainly because of two reasons we can not pass the entire dataset to the network for the training purpose at once. Firstly, datasets by nature most of the time are huge. Let it be image data or some other textual data, in many scenarios it is not possible to feed all the data because of hardware constraints, such as not enough RAM to hold entire dataset.
Secondly to update weight during training process, network has to wait for a very very long time to calculate the delta weight after processing all the input data. To solve this problem usually the full dataset is split into several small batches and number of samples within each small batch is known as 
Batch and batch size.
\subsection{Iterations:} 
Number of batches that a neural network process to complete a single epoch.
The number of  iterations, batch size and  number of data samples in the dataset is given by, the below expression.
\begin{equation}
    D = B * I
\end{equation}
Where D is the total number of samples in the dataset.
\\*B is the number of samples in the mini batch that is fed to the network at once.
\\*I is the  total number of batches the network process in a single epoch.

Larger batch size requires more computational resource and achieves faster completion, in contrast smaller batch size leads to more generalization. In this regards, Yann  LeCun humorously said
\begin{quote}
``Training with large mini batches is bad for your health. More importantly, it's bad for your test error.  Friends, don't let friends use mini batches larger than 32.''
\end{quote}   
The empirical study of the performance of mini-batch stochastic gradient descent in \cite{masters2018revisiting} show that, the team obtained the best training stability and generalization performance using small batch sizes, for a given computational cost, across a wide range of experiments they conducted. In all cases they have achieved the best results with batch sizes m = 32 or smaller.
\subsection{Intersection Over Union:}
Intersection Over Union (IOU) is a measure similar to Jaccard Index that measures the overlap ratio between two bounding boxes. It evaluates the ratio between area of  overlapping area between two BBs and area of union between them.

\begin{equation}
    IoU = \frac{area\: of\: overlap} {area\: of\: union} =
\end{equation}

\begin{center}
\begin{tikzpicture}
\draw [red] (0, 0) rectangle (2, 2);
\draw [blue] (1.0, 1.0) rectangle (3.0, 3.0);
\begin{scope}
	\fill[green] (1.03, 1.03) rectangle (1.97, 1.97);
\end{scope}
\end{tikzpicture}

\begin{tikzpicture}
    \draw[line width=5pt,fill=black] (0,2) -- (6,2);
\end{tikzpicture}\vspace{0.2cm}%

\begin{tikzpicture}
    \filldraw [draw=red,fill=green] (0, 0) rectangle (2, 2);
    \filldraw [draw=blue,fill=green] (1.0, 1.0) rectangle (3.0, 3.0);
    %\begin{scope}
			\fill[green] (0.98, 0.98) rectangle (2.0, 2.0);
		%\end{scope}
\end{tikzpicture}
\end{center}

